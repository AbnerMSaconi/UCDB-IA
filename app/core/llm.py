# app/core/llm.py - Vers√£o com Stop Token Atualizado
from langchain_core.language_models.llms import LLM
from langchain_core.callbacks import CallbackManagerForLLMRun
from typing import Any, List, Optional
import requests
from app.utils.logger import logger
from app.core.config import settings

class LlamaServerLLM(LLM):
    @property
    def _llm_type(self) -> str:
        return "llama_server"

    def _call(
        self,
        prompt: str,
        stop: Optional[List[str]] = None,
        run_manager: Optional[CallbackManagerForLLMRun] = None,
        **kwargs: Any,
    ) -> str:
 
        try:
            logger.info(f"‚Üí Enviando prompt ({len(prompt)} chars)")
            
            # Garanta que o LLM est√° acess√≠vel
            try:
                health = requests.get(f"{settings.LLM_BASE_URL}/models", timeout=5)
                
                if health.status_code != 200:
                    logger.critical("üõë LLM n√£o est√° saud√°vel")
                    raise Exception("LLM n√£o respondeu em /models")
            except Exception as e:
                logger.critical(f"üõë LLM n√£o acess√≠vel: {e}")
             
                raise Exception("LLM n√£o est√° rodando em http://localhost:8080")

            # --- LISTA DE STOP TOKENS ATUALIZADA ---
            stop_tokens = stop or ["Pergunta:", "Resposta:", "|end|", "<<EOT>>"]

            response = requests.post(
                f"{settings.LLM_BASE_URL}/completions",
                json={
                    "prompt": prompt,
                    "temperature": settings.TEMPERATURE,
   
                    "max_tokens": settings.MAX_TOKENS,
                    "top_p": settings.TOP_P,
                    "repeat_penalty": settings.REPETITION_PENALTY,
                    "stop": stop_tokens, # <-- ALTERA√á√ÉO IMPORTANTE AQUI
            
                    "stream": False,
                    "top_k": 40,
                    "min_p": 0.05,
                    "typical_p": 1.0,
                    "frequency_penalty": 0.0, # Zerado para evitar que penalize a repeti√ß√£o de termos t√©cnicos
                    "presence_penalty": 0.0   # Zerado para evitar que penalize a repeti√ß√£o de termos t√©cnicos
                },
                timeout=1200
            )

            if response.status_code != 200:
                logger.error(f"‚ùå HTTP {response.status_code}: {response.text}")
                raise Exception(f"Erro HTTP: {response.status_code}")

            data = response.json()
            
            if "choices" not in data or len(data["choices"]) == 0:
                logger.error("‚ùå Resposta sem 'choices'")
                return "Erro: resposta vazia do LLM"

            text = data["choices"][0]["text"].strip()
            
            if not text:
                logger.warning("‚ö†Ô∏è LLM retornou texto vazio")
                return "Desculpe, o modelo n√£o gerou uma resposta."

            logger.info(f"‚Üê Resposta recebida ({len(text)} chars)")
            return text

        except requests.exceptions.ConnectionError:
            logger.critical("üõë LLM n√£o acess√≠vel. Verifique se est√° rodando em http://localhost:8080")
            raise Exception("LLM n√£o est√° respondendo")
        except Exception as e:
            logger.error(f"‚ùå Erro no LLM: {e}")
          
            raise Exception(f"Erro ao chamar LLM: {str(e)}")

    @property
    def _identifying_params(self) -> dict[str, Any]:
        return {"endpoint": settings.LLM_BASE_URL}