# UCDB Chat üß†üí¨

Bem-vindo ao UCDB Chat, um assistente de estudos acad√©mico inteligente, projetado para responder a perguntas complexas com base num conjunto de documentos PDF fornecidos. Este projeto utiliza uma arquitetura **RAG (Retrieval-Augmented Generation)** para combinar o poder de um Modelo de Linguagem Grande (LLM) local com a informa√ß√£o espec√≠fica dos seus documentos.

## ‚ú® Funcionalidades Principais

  * **Arquitetura RAG:** As respostas s√£o baseadas em factos extra√≠dos diretamente dos seus documentos, minimizando alucina√ß√µes.
  * **LLM Local:** Executa um Modelo de Linguagem Grande (LLM) localmente usando `llama.cpp`, garantindo total privacidade e controlo.
  * **Interface Web Intuitiva:** Um frontend de chat simples e limpo que exibe as respostas em tempo real (*streaming*).
  * **Suporte a Markdown e LaTeX:** As respostas s√£o formatadas com Markdown e suportam f√≥rmulas matem√°ticas via MathJax.
  * **Identifica√ß√£o de Fontes:** Cada resposta inclui refer√™ncias aos documentos e p√°ginas de onde a informa√ß√£o foi extra√≠da.
  * **Hist√≥rico de Conversa:** O assistente "lembra-se" das perguntas anteriores na mesma sess√£o para fornecer respostas contextuais.

## ‚öôÔ∏è Tecnologias Utilizadas

  * **Backend:** FastAPI, Uvicorn, LangChain, Pydantic
  * **Base de Dados Vetorial:** FAISS
  * **Frontend:** HTML5, CSS3, JavaScript (com `marked.js` e `MathJax`)
  * **Servidor de Infer√™ncia:** LLaMA.cpp

## üìÇ Estrutura do Projeto

```
ucdb-ia/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ routes.py       # Endpoints da API (FastAPI)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ schemas.py      # Modelos de dados (Pydantic)
‚îÇ   ‚îú‚îÄ‚îÄ core/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py       # Configura√ß√µes globais da aplica√ß√£o
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py   # Integra√ß√£o com o modelo de embedding
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py          # Integra√ß√£o com o servidor do LLM
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ rag.py          # L√≥gica principal do RAG
‚îÇ   ‚îî‚îÄ‚îÄ utils/
‚îÇ       ‚îî‚îÄ‚îÄ logger.py       # Configura√ß√£o do sistema de logs
‚îú‚îÄ‚îÄ embeddings/             # (Gerado automaticamente) Base de dados vetorial FAISS
‚îú‚îÄ‚îÄ logs/                   # (Gerado automaticamente) Ficheiros de log
‚îú‚îÄ‚îÄ pdfs/                   # Coloque os seus PDFs aqui
‚îú‚îÄ‚îÄ static/                 # Ficheiros do frontend
‚îÇ   ‚îú‚îÄ‚îÄ assets/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ css/style.css
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ js/script.js
‚îÇ   ‚îî‚îÄ‚îÄ index.html
‚îú‚îÄ‚îÄ .env.example            # Exemplo de ficheiro de configura√ß√£o
‚îú‚îÄ‚îÄ main.py                 # Ponto de entrada para iniciar o servidor web
‚îî‚îÄ‚îÄ requirements.txt        # Depend√™ncias Python
```


### Diret√≥rio `app/` - O Cora√ß√£o da Aplica√ß√£o

#### `app/main.py`
Este ficheiro √© respons√°vel por criar e configurar a inst√¢ncia principal da aplica√ß√£o FastAPI.

`create_app() -> FastAPI:`
- Responsabilidade: Inicializa a aplica√ß√£o.
- A√ß√µes:
  - Chama `setup_logging()` para configurar o sistema de logs.
  - Cria a inst√¢ncia do FastAPI.
  - Adiciona `SessionMiddleware` para gerir sess√µes de utilizador e o hist√≥rico de conversas.
  - Adiciona `CORSMiddleware` para permitir que o frontend (a correr em `localhost:8000`) se comunique com o backend.
  - Inclui as rotas definidas em `app.api.routes`.
  - Configura o diret√≥rio `static/` para servir os ficheiros do frontend (HTML, CSS, JS).

`startup()`:
- Responsabilidade: Executa uma a√ß√£o quando a aplica√ß√£o arranca.
- A√ß√µes: Regista uma mensagem informativa no log a indicar que o servidor foi iniciado.

#### `app/core/config.py`
Este ficheiro centraliza todas as configura√ß√µes da aplica√ß√£o usando a biblioteca Pydantic.

`class Settings(BaseSettings):`
- Responsabilidade: Define e carrega todas as vari√°veis de configura√ß√£o a partir de um ficheiro `.env` ou de valores padr√£o.
- Par√¢metros Principais:
  - `LLM_BASE_URL`: O endere√ßo do servidor `llama-server`.
  - `MAX_TOKENS`: O n√∫mero m√°ximo de tokens que o LLM pode gerar numa √∫nica resposta.
  - `TEMPERATURE`, `TOP_P`, `REPETITION_PENALTY`: Par√¢metros que controlam a criatividade, diversidade e o n√≠vel de repeti√ß√£o das respostas do LLM.
  - `CHUNK_SIZE`, `CHUNK_OVERLAP`: Define o tamanho dos peda√ßos de texto e a sobreposi√ß√£o entre eles durante a indexa√ß√£o dos PDFs.
- Propriedades (`@property`):
  - `vectorstore_path`, `pdf_path`, `static_path`: Fun√ß√µes que geram os caminhos absolutos para os diret√≥rios importantes, garantindo que as pastas s√£o criadas se n√£o existirem.

#### `app/core/llm.py`
Este ficheiro cont√©m a classe que se integra com o servidor `llama.cpp`.

`class LlamaServerLLM(LLM):`
- Responsabilidade: Implementa a interface da LangChain para um LLM, permitindo que a nossa aplica√ß√£o se comunique com o `llama-server`.

`_call(...) -> str:`
- A√ß√µes:
  - Define a lista de `stop_tokens`, que s√£o palavras ou s√≠mbolos que indicam ao LLM para parar de gerar texto.
  - Envia um pedido POST para o endpoint `/completions` do `llama-server`, contendo o prompt e todos os par√¢metros de gera√ß√£o definidos no `config.py`.
  - Processa a resposta JSON, extrai o texto gerado e retorna-o.

#### `app/core/embeddings.py`
Semelhante ao `llm.py`, este ficheiro integra-se com o servidor `llama.cpp` para gerar embeddings.

`class LlamaEmbeddings(Embeddings):`
- Responsabilidade: Implementa a interface da LangChain para um modelo de embedding.
- `embed_documents(...)`: Recebe uma lista de textos e faz um pedido ao servidor de embeddings para converter cada texto num vetor.
- `embed_query(...)`: Recebe uma √∫nica string (a pergunta do utilizador) e converte-a num vetor.

#### `app/core/rag.py`
Este √© o ficheiro mais importante, onde toda a l√≥gica do RAG √© implementada.

`_gerar_titulo_para_documento(...)`:
- Responsabilidade: Usa o LLM para ler o in√≠cio de um novo PDF e gerar um t√≠tulo descritivo que represente a sua √°rea de conhecimento.

`_carregar_manifesto(...)` e `_salvar_manifesto(...)`:
- Responsabilidade: Fun√ß√µes auxiliares para ler e escrever no ficheiro `manifest.json`, que armazena a rela√ß√£o entre os nomes dos ficheiros PDF e os seus t√≠tulos gerados.

`_processar_novos_pdfs(...)`:
- Responsabilidade: Carrega novos PDFs, gera os seus t√≠tulos e divide o seu conte√∫do em chunks.

`criar_vectorstore()`:
- Responsabilidade: Orquestra a cria√ß√£o ou atualiza√ß√£o da base de dados vetorial FAISS.
- A√ß√µes:
  - Verifica se existem novos PDFs na pasta `/pdfs` que ainda n√£o foram processados.
  - Se a base de dados j√° existe, carrega-a e adiciona apenas os novos documentos.
  - Se n√£o existe, processa todos os PDFs, gera os seus embeddings e salva a nova base de dados na pasta `/embeddings`.

`criar_rag_chain(...)`:
- Responsabilidade: Cria e configura o `ConversationalRetrievalChain` da LangChain.
- A√ß√µes:
  - Define o `qa_template`, que √© o prompt detalhado com todas as instru√ß√µes para o LLM.
  - Instancia o `LlamaServerLLM`.
  - Configura o retriever para usar a base de dados FAISS.
  - Monta e retorna a chain completa, pronta a ser usada.

#### `app/api/routes.py`
Define os endpoints da API que o frontend utiliza.

`_initialize_rag()`:
- Responsabilidade: Fun√ß√£o de inicializa√ß√£o que garante que o sistema RAG (a base de dados vetorial e a chain) √© carregado apenas uma vez quando a aplica√ß√£o arranca.

`@router.get("/")`:
- Responsabilidade: Serve a p√°gina principal da aplica√ß√£o (`index.html`).

`@router.get("/knowledge-areas")`:
- Responsabilidade: Fornece ao frontend a lista de √°reas de conhecimento (os t√≠tulos dos PDFs processados) a partir do ficheiro `manifest.json`.

`@router.post("/chat")`:
- Responsabilidade: √â o endpoint principal que lida com a conversa do chat.
- A√ß√µes:
  - Recebe a mensagem do utilizador.
  - Recupera o hist√≥rico da conversa da sess√£o do utilizador.
  - Chama a `rag_chain` com a pergunta e o hist√≥rico.
  - Aplica fun√ß√µes de limpeza (`_limpar_resposta_llm`, `_remover_duplicacao`) para corrigir poss√≠veis erros na resposta do LLM.
  - Envia a resposta final e as fontes para o frontend atrav√©s de `StreamingResponse`.

#### `app/utils/logger.py`
Configura um sistema de logging robusto com a biblioteca Loguru.

`setup_logging()`:
- Responsabilidade: Define o formato, o n√≠vel (DEBUG, INFO, ERROR) e o destino dos logs (a consola e ficheiros no diret√≥rio `logs/`).

### Diret√≥rio `static/` - A Interface do Utilizador

`index.html`: A estrutura base da p√°gina do chat, que inclui a √°rea de mensagens e o campo de introdu√ß√£o de texto.

`assets/css/style.css`: Cont√©m todo o estilo visual da aplica√ß√£o, definindo as cores, fontes e o layout dos elementos.

`assets/js/script.js`: Cont√©m toda a l√≥gica do frontend.
- `showWelcomeMessage()`: Ao carregar a p√°gina, faz um pedido ao endpoint `/knowledge-areas` e exibe a mensagem de boas-vindas com a lista de t√≥picos.
- `handleSendMessage()`: √â chamada quando o utilizador clica em "Enviar". Envia a pergunta para o endpoint `/chat` e processa a resposta em stream, atualizando a interface √† medida que o texto chega.
- Utiliza a biblioteca `marked.js` para converter o Markdown recebido do backend em HTML e o `MathJax` para renderizar f√≥rmulas matem√°ticas.


## üöÄ Tutorial de Instala√ß√£o e Execu√ß√£o

Siga estes passos para configurar e executar o projeto localmente.

### Pr√©-requisitos

Antes de come√ßar, garanta que tem o seguinte software instalado:

1.  **Python 3.10+**
2.  **LLaMA.cpp:** Siga o [guia oficial](https://github.com/ggerganov/llama.cpp) para compilar o projeto. O essencial √© ter o execut√°vel `server` pronto a usar.

### Passo 1: Configura√ß√£o do Projeto

1.  **Clone o reposit√≥rio:**

    ```bash
    git clone https://github.com/seu-usuario/ucdb-ia.git
    cd ucdb-ia
    ```

2.  **Crie e ative um ambiente virtual:**

    ```bash
    # Linux/macOS
    python3 -m venv venv
    source venv/bin/activate

    # Windows
    python -m venv venv
    venv\Scripts\activate
    ```

3.  **Instale as depend√™ncias Python:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Crie as pastas necess√°rias:**

    ```bash
    mkdir pdfs
    ```

    *As pastas `embeddings` e `logs` ser√£o criadas automaticamente na primeira execu√ß√£o.*

5.  **Adicione os seus documentos:**
    Coloque todos os ficheiros `.pdf` que servir√£o como base de conhecimento dentro da pasta `pdfs/`.

6.  **Configure as vari√°veis de ambiente:**
    Copie o ficheiro de exemplo e renomeie-o para `.env`.

    ```bash
    cp .env.example .env
    ```

    *N√£o s√£o necess√°rias altera√ß√µes no ficheiro `.env` se voc√™ seguir os comandos abaixo.*

### Passo 2: Execu√ß√£o dos Servidores

Para que o chat funcione, precisamos de tr√™s componentes a serem executados em **tr√™s terminais separados**.

#### Terminal 1: Servidor de Embedding

Este servidor √© respons√°vel por converter texto em vetores num√©ricos.

```bash
# Inicie o servidor de embedding na porta 8081
# (substitua pelo nome do seu modelo de embedding, se for diferente)
llama-server -m seu-modelo-de-embedding.gguf --embeddings -ngl 100 --port 8081
# Caso queira reservar VRAM 
llama-server -m seu-modelo-de-embedding.gguf --embeddings -ngl 0 --port 8081
# N√£o √© necess√°rio processar os PDFs instant√¢neamente.
```

#### Terminal 2: Servidor do LLM (O C√©rebro)

Este √© o modelo principal que ir√° gerar as respostas. Recomenda-se o **Llama-3-8B** para um bom equil√≠brio entre performance e qualidade no seu hardware.

```bash
# Inicie o servidor do LLM na porta 8080
# Substitua pelo caminho do seu modelo .gguf
llama-server -m ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf -c 8192 -ngl 100 -fa 1
```

  * `-c 8192`: Define o tamanho do contexto para 8192 tokens, permitindo respostas mais longas.
  * `-ngl 100`: Descarrega o m√°ximo de camadas para a GPU, garantindo a m√°xima velocidade.
  * `-fa 1`: FlashAttention, otimiza√ß√£o que visa acelerar o processo de infer√™ncia e reduzir o consumo de mem√≥ria da GPU, especialmente com sequ√™ncias de texto longas.

#### Terminal 3: Aplica√ß√£o UCDB Chat

Este √© o servidor web da aplica√ß√£o.

```bash
# Certifique-se de que o seu ambiente virtual (venv) est√° ativo
python main.py
```

### Passo 3: Aceder √† Aplica√ß√£o

Ap√≥s iniciar os tr√™s servidores, abra o seu navegador e aceda a:

**http://localhost:8000**

Na primeira execu√ß√£o, o sistema ir√° processar e indexar todos os PDFs. Este processo pode demorar alguns minutos, dependendo do n√∫mero de documentos. Voc√™ pode acompanhar o progresso nos logs do terminal onde a aplica√ß√£o Python est√° a ser executada. Ap√≥s a conclus√£o, o chat estar√° pronto a ser usado!

-----

## üîß Configura√ß√£o Avan√ßada

Pode ajustar o comportamento do LLM editando o ficheiro `app/core/config.py`.

  * `REPETITION_PENALTY`: Aumente este valor (ex: `1.2`) se notar que o modelo est√° a repetir-se.
  * `TEMPERATURE`: Aumente para respostas mais criativas, diminua (ex: `0.5`) para respostas mais factuais e diretas.
  * `RETRIEVAL_K`: O n√∫mero de *chunks* de texto a serem recuperados dos documentos para cada pergunta. Um valor entre 4 e 6 √© geralmente ideal.


## üìÑ Licen√ßa

Este projeto est√° licenciado sob a **Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)**.

![CC BY-NC 4.0](https://i.creativecommons.org/l/by-nc/4.0/88x31.png)

Isto significa que voc√™ √© livre para partilhar e adaptar este trabalho para **fins n√£o comerciais**, desde que d√™ o cr√©dito apropriado aos criadores originais.

**Uso Comercial:**
O uso deste software em projetos ou produtos comerciais √© estritamente proibido sob esta licen√ßa. Para obter uma licen√ßa comercial, por favor, entre em contacto com a equipa de desenvolvimento.
