# UCDB Chat - Sistema de Chat com RAG e Qwen3-14B

> ğŸš€ Chat inteligente baseado em documentos, usando RAG, FastAPI, llama.cpp e Qwen3-14B.

Este sistema permite fazer perguntas sobre documentos PDFs carregados, com respostas ricas, formatadas em Markdown e geradas em tempo real por um modelo de linguagem local (Qwen3-14B).

---

## ğŸ“ Estrutura do Projeto
---
ucdb-chat/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py       # ConfiguraÃ§Ãµes globais
â”‚   â”‚   â”œâ”€â”€ llm.py          # IntegraÃ§Ã£o com llama-server
â”‚   â”‚   â”œâ”€â”€ embeddings.py   # GeraÃ§Ã£o de embeddings
â”‚   â”‚   â””â”€â”€ rag.py          # Sistema RAG com LangChain
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py       # Rotas FastAPI
â”‚   â”‚   â””â”€â”€ schemas.py      # Modelos Pydantic
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ logger.py       # Sistema de logging
â”œâ”€â”€ static/                 # Frontend (HTML, CSS, JS)
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ assets/
â”‚       â”œâ”€â”€ css/style.css
â”‚       â”œâ”€â”€ js/script.js
â”‚       â””â”€â”€ lib/marked.min.js
â”œâ”€â”€ pdfs/                   # â† Adicione seus PDFs aqui
â”œâ”€â”€ embeddings/             # â† Gerado automaticamente (FAISS)
â”œâ”€â”€ logs/                   # â† Logs do sistema
â”œâ”€â”€ .env.example
â”œâ”€â”€ .gitignore
â”œâ”€â”€ main.py                 # Ponto de entrada
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
---

## ğŸ› ï¸ Requisitos

- Python 3.10+
- Modelo Qwen3-14B (formato `.gguf`)
- `llama-server` (do projeto `llama.cpp`)
- `nomic-embed-text-v1.5` (opcional, para embeddings)

---

## ğŸ”§ InstalaÃ§Ã£o

---
# 1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/ucdb-chat.git
cd ucdb-chat

# 2. Crie e ative o ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
venv\Scripts\activate     # Windows

# 3. Instale as dependÃªncias
pip install -r requirements.txt

# 4. Crie as pastas necessÃ¡rias
mkdir pdfs embeddings logs

# 5. Coloque seus PDFs tÃ©cnicos em /pdfs
# Ex: transistores.pdf, eletronica_basica.pdf

##ğŸš€ Como Executar 
#1. Inicie o LLM (em outro terminal) na porta 8080

#2. Inicie o servidor de embeddings (opcional) na porta 8081

#3. Inicie o UCDB Chat
    python main.py
Acesse: http://localhost:8000 

---
##ğŸ“š DocumentaÃ§Ã£o dos MÃ³dulos
---
##ğŸ“„ app/core/config.py 

#FunÃ§Ã£o: Define todas as configuraÃ§Ãµes do sistema. 
#VariÃ¡veis: 

    APP_NAME: Nome do app
    LLM_BASE_URL: URL do llama-server (http://localhost:8080/v1)
    EMBEDDING_API_URL: URL do servidor de embeddings
    MAX_TOKENS: 2048 (respostas longas)
    TEMPERATURE: 0.8 (criatividade)
    CHUNK_SIZE: 812 (tamanho dos pedaÃ§os de texto)
    RETRIEVAL_K: 4 (nÃºmero de chunks recuperados)
     

#Propriedades: 

    .vectorstore_path: Caminho para embeddings/
    .pdf_path: Caminho para pdfs/
    .static_path: Caminho para static/
     

 
#ğŸ“„ app/core/embeddings.py 

#Classe: LlamaEmbeddings 

#Gera embeddings usando o llama-server em :8081/embedding. 
#MÃ©todos: 

    embed_documents(texts): Gera embeddings para mÃºltiplos textos
    embed_query(text): Gera embedding para uma consulta
     

 
#ğŸ“„ app/core/llm.py 

#Classe: LlamaServerLLM 

#IntegraÃ§Ã£o com o llama-server em :8080. 
#MÃ©todos: 

    _call(prompt): Envia prompt ao LLM e retorna resposta
    Usa requests para POST em /completions
    ParÃ¢metros: max_tokens, temperature, stop, etc.
     

 
#ğŸ“„ app/core/rag.py 

#FunÃ§Ãµes: 
criar_vectorstore() 

    Carrega PDFs com PyPDFLoader
    Divide em chunks com RecursiveCharacterTextSplitter
    Gera embeddings e salva em embeddings/ com FAISS
    Retorna vectorstore (FAISS)
     

criar_rag_chain(vectorstore) 

    Cria RetrievalQA com LangChain
    Usa prompt com {context} e {question}
    Retorna chain pronta para perguntas
     

 
ğŸ“„ app/api/schemas.py 

Modelo: ChatRequest 

    message: str: Mensagem do usuÃ¡rio
     

Usado para validaÃ§Ã£o de entrada na rota /chat. 
 
ğŸ“„ app/api/routes.py 

Rotas FastAPI: 
GET / 

    Retorna index.html
     

POST /chat 

    Recebe mensagem do usuÃ¡rio
    Usa RAG para gerar resposta
    Streaming caractere por caractere com SSE
    Formato:  {"type": "chunk", "content": "OlÃ¡..."}
     

 
ğŸ“„ app/utils/logger.py 

FunÃ§Ã£o: setup_logging() 

    Configura loguru com cores e formato
    Intercepta logs do logging padrÃ£o
    NÃ­veis: INFO, SUCCESS, WARNING, ERROR, CRITICAL
     

 
ğŸ“„ app/main.py 

FunÃ§Ã£o: create_app() 

    Cria instÃ¢ncia do FastAPI
    Adiciona middlewares (CORS, sessÃ£o)
    Monta rotas e arquivos estÃ¡ticos
    Inicia logger
     

 
ğŸ“„ main.py 

Ponto de entrada: 

    Importa app de app.main
    Inicia Uvicorn
    Porta: 8000
    Reload: ativado em dev
     

 
ğŸ–¥ï¸ Frontend (static/) 
index.html 

    PÃ¡gina principal com chat
    Carrega CSS e JS
    Usa EventSource para SSE
     

script.js 

    Streaming de respostas
    Renderiza Markdown com marked.js
    Auto-resize do textarea
    Indicador de digitaÃ§Ã£o
     

style.css 

    Design moderno com gradientes
    Responsivo
    Estilos para cÃ³digo, blocos, etc.
     

 
âš™ï¸ VariÃ¡veis de Ambiente (.env.example) 