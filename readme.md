-----

# UCDB Chat ğŸ§ ğŸ’¬

Bem-vindo ao UCDB Chat, um assistente de estudos acadÃ©mico inteligente, projetado para responder a perguntas complexas com base num conjunto de documentos PDF fornecidos. Este projeto utiliza uma arquitetura **RAG (Retrieval-Augmented Generation)** para combinar o poder de um Modelo de Linguagem Grande (LLM) local com a informaÃ§Ã£o especÃ­fica dos seus documentos.

## âœ¨ Funcionalidades Principais

  * **Arquitetura RAG:** As respostas sÃ£o baseadas em factos extraÃ­dos diretamente dos seus documentos, minimizando alucinaÃ§Ãµes.
  * **LLM Local:** Executa um Modelo de Linguagem Grande (LLM) localmente usando `llama.cpp`, garantindo total privacidade e controlo.
  * **Interface Web Intuitiva:** Um frontend de chat simples e limpo que exibe as respostas em tempo real (*streaming*).
  * **Suporte a Markdown e LaTeX:** As respostas sÃ£o formatadas com Markdown e suportam fÃ³rmulas matemÃ¡ticas via MathJax.
  * **IdentificaÃ§Ã£o de Fontes:** Cada resposta inclui referÃªncias aos documentos e pÃ¡ginas de onde a informaÃ§Ã£o foi extraÃ­da.
  * **HistÃ³rico de Conversa:** O assistente "lembra-se" das perguntas anteriores na mesma sessÃ£o para fornecer respostas contextuais.

## âš™ï¸ Tecnologias Utilizadas

  * **Backend:** FastAPI, Uvicorn, LangChain, Pydantic
  * **Base de Dados Vetorial:** FAISS
  * **Frontend:** HTML5, CSS3, JavaScript (com `marked.js` e `MathJax`)
  * **Servidor de InferÃªncia:** LLaMA.cpp

## ğŸ“‚ Estrutura do Projeto

```
ucdb-ia/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py       # Endpoints da API (FastAPI)
â”‚   â”‚   â””â”€â”€ schemas.py      # Modelos de dados (Pydantic)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py       # ConfiguraÃ§Ãµes globais da aplicaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ embeddings.py   # IntegraÃ§Ã£o com o modelo de embedding
â”‚   â”‚   â”œâ”€â”€ llm.py          # IntegraÃ§Ã£o com o servidor do LLM
â”‚   â”‚   â””â”€â”€ rag.py          # LÃ³gica principal do RAG
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py       # ConfiguraÃ§Ã£o do sistema de logs
â”œâ”€â”€ embeddings/             # (Gerado automaticamente) Base de dados vetorial FAISS
â”œâ”€â”€ logs/                   # (Gerado automaticamente) Ficheiros de log
â”œâ”€â”€ pdfs/                   # Coloque os seus PDFs aqui
â”œâ”€â”€ static/                 # Ficheiros do frontend
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ css/style.css
â”‚   â”‚   â””â”€â”€ js/script.js
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .env.example            # Exemplo de ficheiro de configuraÃ§Ã£o
â”œâ”€â”€ main.py                 # Ponto de entrada para iniciar o servidor web
â””â”€â”€ requirements.txt        # DependÃªncias Python
```

-----

## ğŸš€ Tutorial de InstalaÃ§Ã£o e ExecuÃ§Ã£o

Siga estes passos para configurar e executar o projeto localmente.

### PrÃ©-requisitos

Antes de comeÃ§ar, garanta que tem o seguinte software instalado:

1.  **Python 3.10+**
2.  **LLaMA.cpp:** Siga o [guia oficial](https://github.com/ggerganov/llama.cpp) para compilar o projeto. O essencial Ã© ter o executÃ¡vel `server` pronto a usar.

### Passo 1: ConfiguraÃ§Ã£o do Projeto

1.  **Clone o repositÃ³rio:**

    ```bash
    git clone https://github.com/seu-usuario/ucdb-ia.git
    cd ucdb-ia
    ```

2.  **Crie e ative um ambiente virtual:**

    ```bash
    # Linux/macOS
    python3 -m venv venv
    source venv/bin/activate

    # Windows
    python -m venv venv
    venv\Scripts\activate
    ```

3.  **Instale as dependÃªncias Python:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Crie as pastas necessÃ¡rias:**

    ```bash
    mkdir pdfs
    ```

    *As pastas `embeddings` e `logs` serÃ£o criadas automaticamente na primeira execuÃ§Ã£o.*

5.  **Adicione os seus documentos:**
    Coloque todos os ficheiros `.pdf` que servirÃ£o como base de conhecimento dentro da pasta `pdfs/`.

6.  **Configure as variÃ¡veis de ambiente:**
    Copie o ficheiro de exemplo e renomeie-o para `.env`.

    ```bash
    cp .env.example .env
    ```

    *NÃ£o sÃ£o necessÃ¡rias alteraÃ§Ãµes no ficheiro `.env` se vocÃª seguir os comandos abaixo.*

### Passo 2: ExecuÃ§Ã£o dos Servidores

Para que o chat funcione, precisamos de trÃªs componentes a serem executados em **trÃªs terminais separados**.

#### Terminal 1: Servidor de Embedding

Este servidor Ã© responsÃ¡vel por converter texto em vetores numÃ©ricos.

```bash
# Inicie o servidor de embedding na porta 8081
# (substitua pelo nome do seu modelo de embedding, se for diferente)
llama-server -m seu-modelo-de-embedding.gguf -e -ngl 100 --port 8081
```

#### Terminal 2: Servidor do LLM (O CÃ©rebro)

Este Ã© o modelo principal que irÃ¡ gerar as respostas. Recomenda-se o **Llama-3-8B** para um bom equilÃ­brio entre performance e qualidade no seu hardware.

```bash
# Inicie o servidor do LLM na porta 8080
# Substitua pelo caminho do seu modelo .gguf
llama-server -m ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf -c 8192 -ngl 100 --flash-attn
```

  * `-c 8192`: Define o tamanho do contexto para 8192 tokens, permitindo respostas mais longas.
  * `-ngl 100`: Descarrega o mÃ¡ximo de camadas para a GPU, garantindo a mÃ¡xima velocidade.

#### Terminal 3: AplicaÃ§Ã£o UCDB Chat

Este Ã© o servidor web da aplicaÃ§Ã£o.

```bash
# Certifique-se de que o seu ambiente virtual (venv) estÃ¡ ativo
python main.py
```

### Passo 3: Aceder Ã  AplicaÃ§Ã£o

ApÃ³s iniciar os trÃªs servidores, abra o seu navegador e aceda a:

**http://localhost:8000**

Na primeira execuÃ§Ã£o, o sistema irÃ¡ processar e indexar todos os PDFs. Este processo pode demorar alguns minutos, dependendo do nÃºmero de documentos. VocÃª pode acompanhar o progresso nos logs do terminal onde a aplicaÃ§Ã£o Python estÃ¡ a ser executada. ApÃ³s a conclusÃ£o, o chat estarÃ¡ pronto a ser usado\!

-----

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

Pode ajustar o comportamento do LLM editando o ficheiro `app/core/config.py`.

  * `REPETITION_PENALTY`: Aumente este valor (ex: `1.2`) se notar que o modelo estÃ¡ a repetir-se.
  * `TEMPERATURE`: Aumente para respostas mais criativas, diminua (ex: `0.5`) para respostas mais factuais e diretas.
  * `RETRIEVAL_K`: O nÃºmero de *chunks* de texto a serem recuperados dos documentos para cada pergunta. Um valor entre 4 e 6 Ã© geralmente ideal.

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas\! Se encontrar um bug ou tiver uma sugestÃ£o, por favor, abra uma *issue* no repositÃ³rio.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a LicenÃ§a MIT.
