# UCDB Chat ğŸ§ ğŸ’¬  
> Sistema de chat inteligente baseado em **RAG (Retrieval-Augmented Generation)**, **FastAPI**, **llama.cpp** e o modelo **Qwen3-14B**.  

Permite fazer perguntas sobre **documentos PDF** carregados, retornando respostas formatadas em **Markdown**, geradas em tempo real por um LLM local.  

---

## ğŸ“‚ Estrutura do Projeto
```
ucdb-chat/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py       # ConfiguraÃ§Ãµes globais
â”‚   â”‚   â”œâ”€â”€ llm.py          # IntegraÃ§Ã£o com llama-server
â”‚   â”‚   â”œâ”€â”€ embeddings.py   # GeraÃ§Ã£o de embeddings
â”‚   â”‚   â””â”€â”€ rag.py          # Sistema RAG com LangChain
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py       # Rotas FastAPI
â”‚   â”‚   â””â”€â”€ schemas.py      # Modelos Pydantic
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py       # Sistema de logging
â”œâ”€â”€ static/                 # Frontend (HTML, CSS, JS)
â”‚   â”œâ”€â”€ index.html
â”‚   â””â”€â”€ assets/
â”‚       â”œâ”€â”€ css/style.css
â”‚       â”œâ”€â”€ js/script.js
â”‚       â””â”€â”€ lib/marked.min.js
â”œâ”€â”€ pdfs/                   # PDFs adicionados pelo usuÃ¡rio
â”œâ”€â”€ embeddings/             # Vetores gerados automaticamente (FAISS)
â”œâ”€â”€ logs/                   # Arquivos de log
â”œâ”€â”€ .env.example
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ main.py                 # Ponto de entrada
â””â”€â”€ README.md
```

---

## âš™ï¸ Requisitos
- **Python 3.10+**
- Modelo **Qwen3-14B** no formato `.gguf`
- **llama-server** (via [llama.cpp](https://github.com/ggerganov/llama.cpp))
- (Opcional) `nomic-embed-text-v1.5` para embeddings

---

## ğŸš€ InstalaÃ§Ã£o

```bash
# 1. Clone o repositÃ³rio
git clone https://github.com/seu-usuario/ucdb-chat.git
cd ucdb-chat

# 2. Crie e ative o ambiente virtual
python -m venv venv
source venv/bin/activate   # Linux/Mac
venv\Scriptsctivate      # Windows

# 3. Instale as dependÃªncias
pip install -r requirements.txt

# 4. Crie as pastas necessÃ¡rias
mkdir pdfs embeddings logs

# 5. Coloque seus PDFs em /pdfs
```

---

## â–¶ï¸ Como Executar

1. **Inicie o LLM** (em outro terminal) na porta **8080**  
2. **Inicie o servidor de embeddings** (opcional) na porta **8081**  
3. **Inicie o UCDB Chat**  

```bash
python main.py
```

Acesse em: **http://localhost:8000**

---

## ğŸ“š DocumentaÃ§Ã£o dos MÃ³dulos

### ğŸ”§ `app/core/config.py`
- Define as configuraÃ§Ãµes principais:  
  - `APP_NAME`, `LLM_BASE_URL`, `EMBEDDING_API_URL`, `MAX_TOKENS`, `TEMPERATURE`, etc.  
- Caminhos: `vectorstore_path`, `pdf_path`, `static_path`.  

### ğŸ”§ `app/core/embeddings.py`
- Classe `LlamaEmbeddings`  
  - `embed_documents(texts)` â†’ gera embeddings para documentos  
  - `embed_query(text)` â†’ gera embedding para uma consulta  

### ğŸ”§ `app/core/llm.py`
- Classe `LlamaServerLLM`  
  - IntegraÃ§Ã£o com `llama-server` em `:8080`  
  - MÃ©todo `_call(prompt)` envia o prompt e retorna a resposta  

### ğŸ”§ `app/core/rag.py`
- `criar_vectorstore()` â†’ Carrega PDFs, divide em chunks, gera embeddings e armazena no FAISS  
- `criar_rag_chain(vectorstore)` â†’ Cria pipeline RAG com LangChain  

### ğŸ”§ `app/api/schemas.py`
- Modelo `ChatRequest`:  
  ```python
  message: str
  ```

### ğŸ”§ `app/api/routes.py`
- **GET /** â†’ retorna `index.html`  
- **POST /chat** â†’ recebe mensagem e retorna resposta via SSE  

### ğŸ”§ `app/utils/logger.py`
- ConfiguraÃ§Ã£o avanÃ§ada de logs com **loguru**  

### ğŸ”§ `app/main.py`
- CriaÃ§Ã£o da instÃ¢ncia FastAPI  
- Monta middlewares, rotas e arquivos estÃ¡ticos  

---

## ğŸ’» Frontend (static/)
- `index.html` â†’ pÃ¡gina principal do chat  
- `script.js` â†’ streaming de respostas, markdown renderizado com `marked.js`  
- `style.css` â†’ design responsivo, temas modernos  

---

## ğŸ”‘ VariÃ¡veis de Ambiente (`.env.example`)
```ini
APP_NAME=UCDB Chat
DEBUG=true
MAX_TOKENS=2048
TEMPERATURE=0.8
```

Renomeie para `.env` antes de usar.  

---

## ğŸ§ª Testes

### Testar RAG isolado
```bash
python -c "
from app.core.rag import criar_vectorstore, criar_rag_chain
vectorstore = criar_vectorstore()
chain = criar_rag_chain(vectorstore)
print(chain({'query': 'O que Ã© um transistor?'}))
"
```

### Testar LLM diretamente
```bash
curl http://localhost:8080/v1/completions   -H "Content-Type: application/json"   -d '{"prompt":"Explique transistores.","max_tokens":512}'
```

---

## ğŸ› SoluÃ§Ã£o de Problemas
- **Erro: `BaseSettings` nÃ£o encontrado** â†’ `pip install pydantic-settings`  
- **RAG nÃ£o inicializa** â†’ verifique se hÃ¡ PDFs em `/pdfs`  
- **LLM nÃ£o responde** â†’ teste com `curl http://localhost:8080/v1/models`  
- **Resposta vazia** â†’ aumente `max_tokens` ou ajuste `stop` tokens  
- **Embeddings nÃ£o gerados** â†’ verifique `EMBEDDING_API_URL`  

---

## ğŸ“¦ DependÃªncias Principais
- `fastapi`, `uvicorn` â†’ servidor web  
- `langchain`, `langchain-community` â†’ RAG  
- `pydantic-settings` â†’ configuraÃ§Ãµes  
- `pypdf` â†’ leitura de PDFs  
- `loguru` â†’ logging  

---

## ğŸ¤ ContribuiÃ§Ã£o
1. Fork o projeto  
2. Crie sua branch (`git checkout -b feature/nova-funcao`)  
3. Commit suas mudanÃ§as (`git commit -m 'Adiciona nova funÃ§Ã£o'`)  
4. Push (`git push origin feature/nova-funcao`)  
5. Abra um Pull Request  

---

## ğŸ“„ LicenÃ§a
MIT  