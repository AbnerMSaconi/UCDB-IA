-----

# UCDB Chat ğŸ§ ğŸ’¬

Bem-vindo ao UCDB Chat, um assistente de estudos acadÃ©mico inteligente, projetado para responder a perguntas complexas com base num conjunto de documentos PDF fornecidos. Este projeto utiliza uma arquitetura **RAG (Retrieval-Augmented Generation)** para combinar o poder de um Modelo de Linguagem Grande (LLM) local com a informaÃ§Ã£o especÃ­fica dos seus documentos.

## âœ¨ Funcionalidades Principais

  * **Arquitetura RAG:** As respostas sÃ£o baseadas em factos extraÃ­dos diretamente dos seus documentos, minimizando alucinaÃ§Ãµes.
  * **LLM Local:** Executa um Modelo de Linguagem Grande (LLM) localmente usando `llama.cpp`, garantindo total privacidade e controlo.
  * **Interface Web Intuitiva:** Um frontend de chat simples e limpo que exibe as respostas em tempo real (*streaming*).
  * **Suporte a Markdown e LaTeX:** As respostas sÃ£o formatadas com Markdown e suportam fÃ³rmulas matemÃ¡ticas via MathJax.
  * **IdentificaÃ§Ã£o de Fontes:** Cada resposta inclui referÃªncias aos documentos e pÃ¡ginas de onde a informaÃ§Ã£o foi extraÃ­da.
  * **HistÃ³rico de Conversa:** O assistente "lembra-se" das perguntas anteriores na mesma sessÃ£o para fornecer respostas contextuais.

## âš™ï¸ Tecnologias Utilizadas

  * **Backend:** FastAPI, Uvicorn, LangChain, Pydantic
  * **Base de Dados Vetorial:** FAISS
  * **Frontend:** HTML5, CSS3, JavaScript (com `marked.js` e `MathJax`)
  * **Servidor de InferÃªncia:** LLaMA.cpp

## ğŸ“‚ Estrutura do Projeto
\n---\n\n## ğŸ“¦ Backend\n\n### `main.py` (raiz)\nResponsÃ¡vel por inicializar o servidor FastAPI.\n\n**FunÃ§Ãµes principais:**\n- Importa `app.main` e executa a aplicaÃ§Ã£o.\n- Configura CORS e rota raiz.\n\n**Exemplo de execuÃ§Ã£o:**\n```bash\npython main.py\n```\n\n---\n\n### `app/main.py`\nArquivo central da aplicaÃ§Ã£o.\n\n**FunÃ§Ãµes e componentes:**\n- Cria instÃ¢ncia do FastAPI.\n- Inclui as rotas definidas em `app/api/routes.py`.\n- Carrega configuraÃ§Ãµes globais (porta, host, etc.).\n- Inicializa logs.\n\n---\n\n### `app/api/routes.py`\nDefine as rotas da API (endpoints) utilizadas pelo frontend.\n\n**Principais rotas:**\n| MÃ©todo | Rota     | FunÃ§Ã£o              | DescriÃ§Ã£o                                      |\n|--------|----------|---------------------|------------------------------------------------|\n| GET    | `/`      | `root()`            | Retorna status inicial da API.                 |\n| POST   | `/ask`   | `ask_question()`    | Processa pergunta via RAG e retorna resposta.  |\n| POST   | `/upload`| `upload_pdf()`      | Recebe PDFs e atualiza o Ã­ndice vetorial.      |\n\n**Fluxo interno:**\n- Recebe JSON do frontend.\n- Chama `rag.query(question)` para gerar resposta.\n- Retorna objeto contendo a resposta e fontes relevantes.\n\n---\n\n### `app/api/schemas.py`\nDefine os modelos de dados utilizados pela API via Pydantic.\n\n**Classes principais:**\n```python\nclass QueryRequest(BaseModel):\n    question: str\n\nclass QueryResponse(BaseModel):\n    answer: str\n    sources: list[str]\n\nclass UploadResponse(BaseModel):\n    message: str\n```\nEsses modelos garantem tipagem forte e validaÃ§Ã£o automÃ¡tica das requisiÃ§Ãµes.\n\n---\n\n### `app/api/models.py`\nEstrutura para futuras expansÃµes de persistÃªncia de dados, como:\n- HistÃ³rico de conversas\n- ReferÃªncia de documentos indexados\n- ConfiguraÃ§Ãµes de sessÃ£o\n\n---\n\n### `app/core/config.py`\nGerencia todas as configuraÃ§Ãµes globais do sistema.\n\n**Principais parÃ¢metros:**\n- `TEMPERATURE`: controle da criatividade do modelo\n- `RETRIEVAL_K`: quantidade de trechos recuperados\n- `EMBEDDING_URL`: endereÃ§o do servidor de embeddings\n- `LLM_URL`: endereÃ§o do modelo LLaMA local\n- `LOG_DIR`: diretÃ³rio de logs\n\nAs variÃ¡veis sÃ£o carregadas via `.env` ou valores padrÃ£o.\n\n---\n\n### `app/core/embeddings.py`\nImplementa a geraÃ§Ã£o e o gerenciamento de embeddings para os documentos.\n\n**FunÃ§Ãµes principais:**\n```python\ndef embed_text(text: str) -> np.ndarray:\n    """\n    Envia o texto para o servidor de embeddings e retorna o vetor gerado.\n    """\n\ndef index_documents(pdf_folder: str):\n    """\n    Extrai texto dos PDFs, cria embeddings e salva na base FAISS.\n    """\n\ndef retrieve_similar(query_vector, k=5):\n    """\n    Busca os vetores mais prÃ³ximos no Ã­ndice FAISS com base na consulta.\n    """\n```\n**Fluxo:**\n- Extrai texto dos PDFs\n- Divide em chunks\n- Cria embeddings via servidor local\n- Salva os vetores em FAISS (`.index`)\n- Usa busca vetorial durante a consulta\n\n---\n\n### `app/core/llm.py`\nGerencia a comunicaÃ§Ã£o com o modelo de linguagem LLaMA via servidor REST local.\n\n**FunÃ§Ã£o principal:**\n```python\ndef generate(prompt: str) -> str:\n    """\n    Envia prompt ao servidor LLaMA e retorna a resposta gerada.\n    """\n```\n**Mecanismos de controle:**\n- `temperature`\n- `top_p`\n- `max_tokens`\n- `repetition_penalty`\n\n---\n\n### `app/core/rag.py`\nNÃºcleo do sistema RAG (Retrieval-Augmented Generation).\n\n**FunÃ§Ã£o principal:**\n```python\ndef query(question: str) -> dict:\n    """\n    Executa o pipeline completo:\n    1. Gera embedding da pergunta.\n    2. Busca trechos relevantes na base FAISS.\n    3. ConstrÃ³i prompt contextualizado.\n    4. Chama o modelo LLaMA.\n    5. Retorna a resposta e fontes.\n    """\n```\n**Estrutura:**\n- Combina mÃ³dulos `embeddings.py` e `llm.py`\n- Usa logs e streaming\n- Garante contexto atualizado conforme PDFs enviados\n\n---\n\n### `app/utils/logger.py`\nGerencia o sistema de logs do projeto.\n\n```python\ndef get_logger(name):\n    """\n    Retorna um logger configurado para o mÃ³dulo solicitado.\n    """\n```\nCada mÃ³dulo cria seu prÃ³prio logger, salvando registros no diretÃ³rio `logs/` com timestamp.\n\n---\n\n### `app/utils/streaming.py`\nResponsÃ¡vel pelo streaming de respostas (envio em tempo real ao frontend).\n\nPermite que a resposta do LLaMA seja exibida enquanto Ã© gerada, simulando comportamento de chat contÃ­nuo.\n\n---\n\n## ğŸ’» Frontend\n\nLocalizado em `static/`.\n\n### `index.html`\nEstrutura da interface principal do chat.\n\n**Elementos principais:**\n- Caixa de chat com mensagens do usuÃ¡rio e da IA\n- Campo de texto e botÃ£o â€œEnviarâ€\n- Componente para upload de PDFs\n- Conecta-se Ã  API via JavaScript (`fetch`)\n\n---\n\n### `assets/css/style.css`\nDefine o estilo da interface:\n- Layout responsivo\n- Cores neutras (tema acadÃªmico)\n- Estilo de bolhas de chat\n- AnimaÃ§Ãµes leves de digitaÃ§Ã£o\n\n---\n\n### `assets/js/script.js`\nGerencia toda a lÃ³gica de interaÃ§Ã£o do frontend.\n\n**FunÃ§Ãµes principais:**\n```javascript\nasync function sendMessage() {\n  // LÃª a mensagem do usuÃ¡rio\n  // Exibe no chat\n  // Envia via POST /ask\n  // Exibe a resposta retornada pela API\n}\n\nasync function uploadPDF(file) {\n  // Envia arquivo para /upload\n  // Atualiza Ã­ndice FAISS no servidor\n}\n```\n**Outros comportamentos:**\n- Scroll automÃ¡tico do chat\n- ExibiÃ§Ã£o de mensagens do sistema\n- RenderizaÃ§Ã£o de Markdown e LaTeX\n

```
ucdb-ia/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ routes.py       # Endpoints da API (FastAPI)
â”‚   â”‚   â””â”€â”€ schemas.py      # Modelos de dados (Pydantic)
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ config.py       # ConfiguraÃ§Ãµes globais da aplicaÃ§Ã£o
â”‚   â”‚   â”œâ”€â”€ embeddings.py   # IntegraÃ§Ã£o com o modelo de embedding
â”‚   â”‚   â”œâ”€â”€ llm.py          # IntegraÃ§Ã£o com o servidor do LLM
â”‚   â”‚   â””â”€â”€ rag.py          # LÃ³gica principal do RAG
â”‚   â””â”€â”€ utils/
â”‚       â””â”€â”€ logger.py       # ConfiguraÃ§Ã£o do sistema de logs
â”œâ”€â”€ embeddings/             # (Gerado automaticamente) Base de dados vetorial FAISS
â”œâ”€â”€ logs/                   # (Gerado automaticamente) Ficheiros de log
â”œâ”€â”€ pdfs/                   # Coloque os seus PDFs aqui
â”œâ”€â”€ static/                 # Ficheiros do frontend
â”‚   â”œâ”€â”€ assets/
â”‚   â”‚   â”œâ”€â”€ css/style.css
â”‚   â”‚   â””â”€â”€ js/script.js
â”‚   â””â”€â”€ index.html
â”œâ”€â”€ .env.example            # Exemplo de ficheiro de configuraÃ§Ã£o
â”œâ”€â”€ main.py                 # Ponto de entrada para iniciar o servidor web
â””â”€â”€ requirements.txt        # DependÃªncias Python
```

-----

## ğŸš€ Tutorial de InstalaÃ§Ã£o e ExecuÃ§Ã£o

Siga estes passos para configurar e executar o projeto localmente.

### PrÃ©-requisitos

Antes de comeÃ§ar, garanta que tem o seguinte software instalado:

1.  **Python 3.10+**
2.  **LLaMA.cpp:** Siga o [guia oficial](https://github.com/ggerganov/llama.cpp) para compilar o projeto. O essencial Ã© ter o executÃ¡vel `server` pronto a usar.

### Passo 1: ConfiguraÃ§Ã£o do Projeto

1.  **Clone o repositÃ³rio:**

    ```bash
    git clone https://github.com/seu-usuario/ucdb-ia.git
    cd ucdb-ia
    ```

2.  **Crie e ative um ambiente virtual:**

    ```bash
    # Linux/macOS
    python3 -m venv venv
    source venv/bin/activate

    # Windows
    python -m venv venv
    venv\Scripts\activate
    ```

3.  **Instale as dependÃªncias Python:**

    ```bash
    pip install -r requirements.txt
    ```

4.  **Crie as pastas necessÃ¡rias:**

    ```bash
    mkdir pdfs
    ```

    *As pastas `embeddings` e `logs` serÃ£o criadas automaticamente na primeira execuÃ§Ã£o.*

5.  **Adicione os seus documentos:**
    Coloque todos os ficheiros `.pdf` que servirÃ£o como base de conhecimento dentro da pasta `pdfs/`.

6.  **Configure as variÃ¡veis de ambiente:**
    Copie o ficheiro de exemplo e renomeie-o para `.env`.

    ```bash
    cp .env.example .env
    ```

    *NÃ£o sÃ£o necessÃ¡rias alteraÃ§Ãµes no ficheiro `.env` se vocÃª seguir os comandos abaixo.*

### Passo 2: ExecuÃ§Ã£o dos Servidores

Para que o chat funcione, precisamos de trÃªs componentes a serem executados em **trÃªs terminais separados**.

#### Terminal 1: Servidor de Embedding

Este servidor Ã© responsÃ¡vel por converter texto em vetores numÃ©ricos.

```bash
# Inicie o servidor de embedding na porta 8081
# (substitua pelo nome do seu modelo de embedding, se for diferente)
llama-server -m seu-modelo-de-embedding.gguf -e -ngl 100 --port 8081
```

#### Terminal 2: Servidor do LLM (O CÃ©rebro)

Este Ã© o modelo principal que irÃ¡ gerar as respostas. Recomenda-se o **Llama-3-8B** para um bom equilÃ­brio entre performance e qualidade no seu hardware.

```bash
# Inicie o servidor do LLM na porta 8080
# Substitua pelo caminho do seu modelo .gguf
llama-server -m ./Meta-Llama-3-8B-Instruct.Q4_K_M.gguf -c 8192 -ngl 100 --flash-attn
```

  * `-c 8192`: Define o tamanho do contexto para 8192 tokens, permitindo respostas mais longas.
  * `-ngl 100`: Descarrega o mÃ¡ximo de camadas para a GPU, garantindo a mÃ¡xima velocidade.

#### Terminal 3: AplicaÃ§Ã£o UCDB Chat

Este Ã© o servidor web da aplicaÃ§Ã£o.

```bash
# Certifique-se de que o seu ambiente virtual (venv) estÃ¡ ativo
python main.py
```

### Passo 3: Aceder Ã  AplicaÃ§Ã£o

ApÃ³s iniciar os trÃªs servidores, abra o seu navegador e aceda a:

**http://localhost:8000**

Na primeira execuÃ§Ã£o, o sistema irÃ¡ processar e indexar todos os PDFs. Este processo pode demorar alguns minutos, dependendo do nÃºmero de documentos. VocÃª pode acompanhar o progresso nos logs do terminal onde a aplicaÃ§Ã£o Python estÃ¡ a ser executada. ApÃ³s a conclusÃ£o, o chat estarÃ¡ pronto a ser usado\!

-----

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

Pode ajustar o comportamento do LLM editando o ficheiro `app/core/config.py`.

  * `REPETITION_PENALTY`: Aumente este valor (ex: `1.2`) se notar que o modelo estÃ¡ a repetir-se.
  * `TEMPERATURE`: Aumente para respostas mais criativas, diminua (ex: `0.5`) para respostas mais factuais e diretas.
  * `RETRIEVAL_K`: O nÃºmero de *chunks* de texto a serem recuperados dos documentos para cada pergunta. Um valor entre 4 e 6 Ã© geralmente ideal.

## ğŸ¤ ContribuiÃ§Ã£o

ContribuiÃ§Ãµes sÃ£o bem-vindas\! Se encontrar um bug ou tiver uma sugestÃ£o, por favor, abra uma *issue* no repositÃ³rio.

## ğŸ“„ LicenÃ§a

Este projeto estÃ¡ licenciado sob a **Creative Commons Attribution-NonCommercial 4.0 International (CC BY-NC 4.0)**.

![CC BY-NC 4.0](https://i.creativecommons.org/l/by-nc/4.0/88x31.png)

Isto significa que vocÃª Ã© livre para partilhar e adaptar este trabalho para **fins nÃ£o comerciais**, desde que dÃª o crÃ©dito apropriado aos criadores originais.

**Uso Comercial:**
O uso deste software em projetos ou produtos comerciais Ã© estritamente proibido sob esta licenÃ§a. Para obter uma licenÃ§a comercial, por favor, entre em contacto com a equipa de desenvolvimento.
